{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1cgJVaU3NOMzq6wtYdN4-2mBykOfw7NDP",
      "authorship_tag": "ABX9TyP3XjChHhYWdgzrDBV0MV4i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayupal25/Text-Generation-Model-LSTM/blob/main/Text_Generation_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "N2d4ZLJUUbON"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"Wonderland.txt\"\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()"
      ],
      "metadata": {
        "id": "i0TXflNfWB-Y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "metadata": {
        "id": "2XxL8G7dWFYI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3giRUxHWQyh",
        "outputId": "0422a474-193e-4423-9c0e-a4d062b53b12"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Characters:  164014\n",
            "Total Vocab:  63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        " seq_in = raw_text[i:i + seq_length]\n",
        " seq_out = raw_text[i + seq_length]\n",
        " dataX.append([char_to_int[char] for char in seq_in])\n",
        " dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_r6OvgHYxBJ",
        "outputId": "476bef44-7575-4d53-8b5b-83f141af28bd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Patterns:  163914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "X = X / float(n_vocab)\n",
        "y = to_categorical(dataY)"
      ],
      "metadata": {
        "id": "KkaBGmt5ZVhe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "aZ7TBhqsZe3m"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "metadata": {
        "id": "aOsSUEITZ5Dl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-fAiFrPaFrR",
        "outputId": "2fe98b1c-6562-4801-ccf6-7f677426ea80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "2561/2562 [============================>.] - ETA: 0s - loss: 2.8592\n",
            "Epoch 1: loss improved from inf to 2.85921, saving model to weights-improvement-01-2.8592-bigger.hdf5\n",
            "2562/2562 [==============================] - 56s 18ms/step - loss: 2.8592\n",
            "Epoch 2/50\n",
            "   7/2562 [..............................] - ETA: 43s - loss: 2.6960"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2561/2562 [============================>.] - ETA: 0s - loss: 2.5467\n",
            "Epoch 2: loss improved from 2.85921 to 2.54672, saving model to weights-improvement-02-2.5467-bigger.hdf5\n",
            "2562/2562 [==============================] - 47s 18ms/step - loss: 2.5467\n",
            "Epoch 3/50\n",
            "2562/2562 [==============================] - ETA: 0s - loss: 2.3561\n",
            "Epoch 3: loss improved from 2.54672 to 2.35610, saving model to weights-improvement-03-2.3561-bigger.hdf5\n",
            "2562/2562 [==============================] - 47s 18ms/step - loss: 2.3561\n",
            "Epoch 4/50\n",
            "2562/2562 [==============================] - ETA: 0s - loss: 2.2151\n",
            "Epoch 4: loss improved from 2.35610 to 2.21514, saving model to weights-improvement-04-2.2151-bigger.hdf5\n",
            "2562/2562 [==============================] - 48s 19ms/step - loss: 2.2151\n",
            "Epoch 5/50\n",
            "2560/2562 [============================>.] - ETA: 0s - loss: 2.1116\n",
            "Epoch 5: loss improved from 2.21514 to 2.11152, saving model to weights-improvement-05-2.1115-bigger.hdf5\n",
            "2562/2562 [==============================] - 48s 19ms/step - loss: 2.1115\n",
            "Epoch 6/50\n",
            "2560/2562 [============================>.] - ETA: 0s - loss: 2.0319\n",
            "Epoch 6: loss improved from 2.11152 to 2.03184, saving model to weights-improvement-06-2.0318-bigger.hdf5\n",
            "2562/2562 [==============================] - 48s 19ms/step - loss: 2.0318\n",
            "Epoch 7/50\n",
            "2561/2562 [============================>.] - ETA: 0s - loss: 1.9631\n",
            "Epoch 7: loss improved from 2.03184 to 1.96311, saving model to weights-improvement-07-1.9631-bigger.hdf5\n",
            "2562/2562 [==============================] - 49s 19ms/step - loss: 1.9631\n",
            "Epoch 8/50\n",
            "2560/2562 [============================>.] - ETA: 0s - loss: 1.9074\n",
            "Epoch 8: loss improved from 1.96311 to 1.90744, saving model to weights-improvement-08-1.9074-bigger.hdf5\n",
            "2562/2562 [==============================] - 49s 19ms/step - loss: 1.9074\n",
            "Epoch 9/50\n",
            "2561/2562 [============================>.] - ETA: 0s - loss: 1.8614\n",
            "Epoch 9: loss improved from 1.90744 to 1.86139, saving model to weights-improvement-09-1.8614-bigger.hdf5\n",
            "2562/2562 [==============================] - 49s 19ms/step - loss: 1.8614\n",
            "Epoch 10/50\n",
            "2560/2562 [============================>.] - ETA: 0s - loss: 1.8183\n",
            "Epoch 10: loss improved from 1.86139 to 1.81833, saving model to weights-improvement-10-1.8183-bigger.hdf5\n",
            "2562/2562 [==============================] - 49s 19ms/step - loss: 1.8183\n",
            "Epoch 11/50\n",
            "2561/2562 [============================>.] - ETA: 0s - loss: 1.7795\n",
            "Epoch 11: loss improved from 1.81833 to 1.77951, saving model to weights-improvement-11-1.7795-bigger.hdf5\n",
            "2562/2562 [==============================] - 49s 19ms/step - loss: 1.7795\n",
            "Epoch 12/50\n",
            "2560/2562 [============================>.] - ETA: 0s - loss: 1.7459\n",
            "Epoch 12: loss improved from 1.77951 to 1.74588, saving model to weights-improvement-12-1.7459-bigger.hdf5\n",
            "2562/2562 [==============================] - 49s 19ms/step - loss: 1.7459\n",
            "Epoch 13/50\n",
            "2561/2562 [============================>.] - ETA: 0s - loss: 1.7091\n",
            "Epoch 13: loss improved from 1.74588 to 1.70907, saving model to weights-improvement-13-1.7091-bigger.hdf5\n",
            "2562/2562 [==============================] - 49s 19ms/step - loss: 1.7091\n",
            "Epoch 14/50\n",
            "2562/2562 [==============================] - ETA: 0s - loss: 1.6846\n",
            "Epoch 14: loss improved from 1.70907 to 1.68462, saving model to weights-improvement-14-1.6846-bigger.hdf5\n",
            "2562/2562 [==============================] - 49s 19ms/step - loss: 1.6846\n",
            "Epoch 15/50\n",
            "2560/2562 [============================>.] - ETA: 0s - loss: 1.6567\n",
            "Epoch 15: loss improved from 1.68462 to 1.65674, saving model to weights-improvement-15-1.6567-bigger.hdf5\n",
            "2562/2562 [==============================] - 49s 19ms/step - loss: 1.6567\n",
            "Epoch 16/50\n",
            "2562/2562 [==============================] - ETA: 0s - loss: 1.6340\n",
            "Epoch 16: loss improved from 1.65674 to 1.63395, saving model to weights-improvement-16-1.6340-bigger.hdf5\n",
            "2562/2562 [==============================] - 49s 19ms/step - loss: 1.6340\n",
            "Epoch 17/50\n",
            " 583/2562 [=====>........................] - ETA: 37s - loss: 1.5911"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"weights-improvement-49-1.3389-bigger.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "3wI9ahO4aHIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "metadata": {
        "id": "7J8rXAQsaWR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = np.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "for i in range(1000):\n",
        "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = np.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    sys.stdout.write(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "metadata": {
        "id": "_cVD-gHCabKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seed:\n",
        "\" ect,\n",
        "and she grew no larger: still it was very uncomfortable, and, as there\n",
        "seemed to be no sort of  \"\n",
        "the car  nhe tas ano the wai so the toiee  “hh wou dan  no you toen i sood to the soene t the soie toe toen i sood to the soene to the soie the was oo the car  nh the was an toe tas oo the sar oo the car  she had  nhe toe toen to the soie to tee io the sai so the thit har  and whe kad  nhe toe toen to the toiee to the soiee to the shite tas ano the cad  nhe fad no tai an toe tas oo the sar oo the sar so the soiee an the sas ho the cad    \n",
        "Done."
      ],
      "metadata": {
        "id": "DU5F-VXg9QUB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HM78KjEC9YMs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}